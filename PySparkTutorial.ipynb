{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySparkTutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FjsR9GZemXN9",
        "FSLy8EIT9Qs1",
        "AvnZOVxV7K9t",
        "NKioCZzjlL-L",
        "o_9RPA4ZL7W9",
        "Ef25TfNJkhfs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fuenfgeld/2022TeamADataEngineeringBC/blob/Christians-Branch/PySparkTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0. Data Engineering Bootcamp\n",
        "In this tutorial you will be introduced to an aspect of Data Engineering called ETL. Together we will implement an ETL workflow with Apache Spark in Python. By the end of the tutorial you will be able to adapt such a workflow to your specific needs and know the benefits of using Spark in doing so."
      ],
      "metadata": {
        "id": "-DZ60lbz97U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 0.1 What is Data Engineering ?\n",
        "\n",
        "Data engineering is the practice of designing and building systems for collecting, storing, and analyzing data at scale. Data engineers work in a variety of settings to build systems that collect, manage, and convert raw data into usable information for data scientists and business analysts to interpret. Their ultimate goal is to make data accessible so that organizations can use it to evaluate and optimize their performance. This last sentence also sums up the difference between a data engineer and a data analyst, whereas the former manages the data resources the later exploits them to gain valuable insights.\n",
        "\n",
        "### 0.2 What is ETL ?\n",
        "\n",
        "According to IBM ETL, which stands for extract, transform and load, is a data integration process that combines data from multiple data sources into a single, consistent data store. It is closely linked with the concept of a *Data Warehouse* that describes central repositories of integrated data from one or more disparate sources. \n",
        "\n",
        "#### Extraction\n",
        "During data extraction, raw data is copied or exported from source locations from a variety of data sources, which can be structured or unstructured such as SQL databases, json files or even web pages.\n",
        "#### Transformation\n",
        "The collected raw data then undergoes data processing. Here, the data is transformed and consolidated for its intended analytical use case. Steps taken during transformation are de-duplicating values, performing calculations, translations, or summarizations based on the raw data and changing the shape of the data via joining and grouping operation in order to match the schema of the target data warehouse. The environment in which the transformation step is performed is also called *staging area*.\n",
        "#### Loading\n",
        "In this last step, the transformed data is moved from the staging area into a target data warehouse. Typically, this involves an initial loading of all data, followed by periodic loading of incremental data changes \n",
        "\n",
        "### 0.3 What is Spark ?\n",
        "\n",
        "According to the official website\n",
        "\n",
        ">*Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.*\n",
        "\n",
        "Now what does that mean ? You can think of Spark as a programming library that allows you to outsource your data engineering workflow to a set of servers (cluster) which enables you to parallelize operations, enabling faster execution and the ability to work with amounts of data that couldn't be handled on a single computer (Big Data). \n",
        "\n",
        "Hence what Spark does is managing the interaction between your local  node (computer) and each node (server) of the cluster. Since Spark was originally written in Scala there is no direct way to access its functionality in Python. This is where *PySpark* comes into play. You can think of PySpark as a Python-based wrapper on top of the Scala API. There are also similar wrappers for *R* and other programming languages, this is why the official website describes Spark as a *multi-language engine*. \n",
        "\n",
        "#### SparkSQL\n",
        "\n",
        "Although the *Resilient Distributed Dataset* (RDD) is the  fundamental data structure on which all higher-level data structures are constructed in Spark, this tutorial is going to focus on the *DataFrame* from the SparkSQL model which deals with structured data such as .json and .csv files. \n",
        "\n",
        "The DataFrame has two big advantages over the RDD. First it has  significant performance benefits over RDDs due to a powerful optimization engine and secondly important data science module such as *spark.streaming* an *spark.ml* work with DataFrames instead of RDDs.\n",
        " \n",
        "## 0.4. Conclusion.\n",
        "You learned what data engineering is,how an ETL workflow is structured and what role Spark plays in such a  contex. Now let's get started with coding stuff. \n",
        "\n",
        "The next three lines of code will make Pyspark and all the relevant data you need to finish this tutorial available in your Colab notebook.\n"
      ],
      "metadata": {
        "id": "tidhxKfZlyJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kMaqIMwKk-A"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/ca4b2ecc9e9ee242037d11c27edd4f4ad770e7ee/iris.json"
      ],
      "metadata": {
        "id": "AWFXHurvlAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -cq https://raw.githubusercontent.com/Fuenfgeld/2022TeamADataEngineeringBC/main/iris2.json"
      ],
      "metadata": {
        "id": "UwhFqG5OpkrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Getting Started \n",
        "\n",
        "Let's get started building our first Spark application. At the core of our Spark application is the *SparkSession* object which acts as a point of entry to interact with underlying Spark\n",
        "functionalities.\n",
        "\n",
        "Usually Spark would delegate the computation jobs to the so called *executors* (CPUs in each node of the cluster) through the so called *driver*. Since this notebook isn't connected to a cluster the jobs will be performed locally though. The concept however is still the same just be aware that we aren't exploiting Sparks full capabilities here for practical reasons (Clusters are not trivial to deal with).\n"
      ],
      "metadata": {
        "id": "FjsR9GZemXN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "WQMsUJrIM3-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our Spark session is initialized we can load in some data to work with. Spark can handle data from all kinds of sources such as .json files, .csv files and even access data from AWS or Azure via dedicated interfaces."
      ],
      "metadata": {
        "id": "ExNrRpo4-HYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Schemas and Creating DataFrames\n",
        "The core object of pyspark.sql is the *DataFrame* which stores the information obtained from external sources (.json, .csv files etc.) or created by the user within Python. To the humans eye the DataFrame behaves like a table, computationally however it is a more complex structure due to the distributed nature of spark objects.\n",
        "\n",
        "Before we load in our first dataset in such a DataFrame we should talk about so called *schemas*. When reading in structured data from an external source it is often know to the user how the data is structured, for instance what the names and data types of the columns are. In contrast to the user however Spark doesn't have any idea about what the types and columns are going to look like.\n",
        "\n",
        "Defining and including a schema in the data loading pipeline in line `3` makes Spark aware of the structure of the data before loading it. This has three big advantages:\n",
        "\n",
        "1.   You relieve Spark from the onus of inferring data types.\n",
        "\n",
        "2.   You prevent Spark from creating a separate job just to read a large portion of your file to ascertain the schema, which for a large data file can be expensive and time-consuming.\n",
        "\n",
        "3. You can detect errors early if data doesn’t match the schema.\n",
        "\n",
        "It is therefore encouraged to define a schema upfront especially when dealing with large sources of data. One easy way to define a schema in pyspark is via a so called *Data Definition Language* (DDL) string as shown in line `1`.\n",
        "\n",
        " Since  `iris.json` is a large file with multiple lines we also want to inform PySpark about this fact by adding `.option(\"multiline\",True)` to the pipeline. Although the second option in the pipeline instructing PySpark to make a header isn't necessary for .json files, it is for .csv files and we include it for the sake of completness."
      ],
      "metadata": {
        "id": "zGqOCdfwA36U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"petalLength DOUBLE, petalWidth DOUBLE, sepalLength DOUBLE, sepalWidth DOUBLE, species STRING\"\n",
        "\n",
        "df1 = (spark.read.option(\"multiline\",True)\n",
        "                 .option(\"header\",True)\n",
        "                 .schema(schema)\n",
        "                 .json('iris.json'))\n",
        "\n",
        "print(f\"Object Type: {type(df1)}\\n\")\n",
        "print(\"Column Info:\")\n",
        "df1.printSchema()\n",
        "print(\"Summary Statistics of columns:\")\n",
        "df1.describe().show()\n",
        "print(\"Overview Dataframe:\")\n",
        "df1.show(10)"
      ],
      "metadata": {
        "id": "Kc5OYn6VNa9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might have wondered why we called `.show()` behind `.describe()` in line 6, especially if you are familiar with *Pandas*. The reason is the so called *Lazy Execution* where code is only executed  once so called *actions* are called. The next chapter will begin introducing these concepts in more depth. We will also get to know *transformations*, code that changes the structure and entries of DataFrames."
      ],
      "metadata": {
        "id": "s8_T1h5sC_Sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Actions and Basic Transformations\n",
        "\n",
        "Spark operations on distributed data can be classified into two types: *transformations* and *actions*. Transformations, as the name suggests, transform a Spark DataFrame into a new DataFrame without altering the original data, giving it the property of *immutability*. \n",
        "\n",
        "\n",
        "All transformations are evaluated lazily. That is, their results are not computed immediately, but they are recorded or remembered as a *lineage*. A recorded lineage allows Spark, at a later time in its execution plan, to rearrange certain transformations or optimize them into stages for more efficient execution. \n",
        "\n",
        "Because Spark records each transformation in its lineage and the DataFrames are immutable between transformations, it can reproduce its original state by simply replaying the recorded lineage, giving it resiliency in the event of failures.\n",
        "\n",
        "Now let's set up some simple Data Engineering workflows in PySpark using actions and transformations."
      ],
      "metadata": {
        "id": "92lmS82aSNDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Accessing Rows and Columns\n",
        "\n",
        "Since Spark was concieved to work with distributed data there is no simple way to access rows at will.\n",
        "\n",
        "If you want to do so anyways you have the possibility to pull the data onto your local node.\n",
        "\n",
        "The action `.collect()` collects the distributed data to the driver side as local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side.\n",
        "\n",
        "Other actions to get only a subset of the  data to your drivers side are `.take()` and `.sample()`."
      ],
      "metadata": {
        "id": "FSLy8EIT9Qs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns list of Row objects\n",
        "local_df1 = df1.collect()\n",
        "print(f\"Type of entries: {type(local_df1[0])}\\n\")\n",
        "print(f\"Entries: {local_df1[:5]}\")"
      ],
      "metadata": {
        "id": "wqjHubRZSVJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Accessing columns doesn't come with the difficulties associated with handling rows. If we want to get specific columns we can simply do so through the `.select()` method. "
      ],
      "metadata": {
        "id": "ClcImNjp1pRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.select(\"petalLength\").show(5)"
      ],
      "metadata": {
        "id": "YjXEYeYSPu6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Adding and Removing  Columns\n",
        "\n",
        "In case we want to add columns we can do so via the `.withColumn()` method. Note that we have to specify the name of the column which is in this case `petalSum`. Usually the new column is a function of one or more of the old columns. "
      ],
      "metadata": {
        "id": "AvnZOVxV7K9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df1.withColumn('newColumn', df1.petalWidth + df1.petalLength)\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "id": "7XRQiJVSij7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The name `'newColumn'` isn't really informative. It's therefore hard for the user to deduce that is it the sum of `'petalWidth'` and `'petalLength'`. So why not rename it to something more indicative ? We can do this via the `.withColumnRenamed()` method."
      ],
      "metadata": {
        "id": "Pf1MnUAcI7M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_extraCol = df_extraCol.withColumnRenamed('newColumn','petalSum')\n",
        "df_extraCol.show(5)"
      ],
      "metadata": {
        "id": "AHeB_op4JtMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get rid of our new column `.drop()` can be used. In contrast to `.select()`, this method removes the specified column completely instead of returning it as slice ot the table.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NKioCZzjlL-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df_extraCol.drop(df_extraCol.petalSum)\n",
        "df1.show(5)"
      ],
      "metadata": {
        "id": "Xp8ScsA-ixVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Basic Data Cleaning\n",
        "\n",
        "Just as in the hospital, hygiene is of great importance to working with data, sometimes rows contain entries that make dealing with our data more difficult or lower its quality (information pollution). Two examples come to mind: Duplicate entries could bias introduce into our data which negatively impacts the performance of a lot of machine learning algorithms.\n",
        "\n",
        "The second example would be null entries which might render some rows useless due to the fact that most algorithms generally can't handle such entries. Luckily PySpark provides us with two methods `.dropna()` and `.dropDuplicates()` to get rid of such problematic rows.\n",
        "\n"
      ],
      "metadata": {
        "id": "o_9RPA4ZL7W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"petalWidth\")\n",
        "    .withColumnRenamed(\"petalWidth\", \"pW\")\n",
        "    .dropna()\n",
        "    .dropDuplicates()\n",
        "    .show(n=5))"
      ],
      "metadata": {
        "id": "22OiPKXUTVVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although our dataframe is now free of unwanted entries we might still want to put further restrictions on the data we want to keep. "
      ],
      "metadata": {
        "id": "Em-5JUVf82Kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Conditional Selection of Rows.\n",
        "\n",
        "In 2.1. we explained that directly accessing rows of a DataFrame comes with some caveats, it is however possible to indirectly access rows without pulling all the data onto your local node. This is done via conditional selection where we select rows based on user given conditions via the `.filter()` method. This means however that we don't know which rows we will obtain in the end, hence why we speak of indirect access.\n",
        "\n",
        "Let's say we want to get only the flowers of type `\"virginica\"` we then have to write the following:"
      ],
      "metadata": {
        "id": "RtB5fpk0flQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"species\", \"petalWidth\")\n",
        "    .filter(df1.species == \"virginica\")\n",
        "    .dropDuplicates()\n",
        "    .show(5))             "
      ],
      "metadata": {
        "id": "XN6A0o-ziDhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also filter based on multiple conditions. In the next example we will use the alia `.where()` instead o `.filter()`. Note that both aliases are used with `.where()` coming from SQL syntax."
      ],
      "metadata": {
        "id": "fNZTni8DBha5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"species\", \"petalWidth\", \"petalLength\")\n",
        "    .where((df1.species == \"setosa\") & (df1.petalLength > 1.3))\n",
        "    .withColumn(\"petalSum\", df1.petalWidth + df1.petalLength)\n",
        "    .dropDuplicates()\n",
        "    .describe()\n",
        "    .show(5))"
      ],
      "metadata": {
        "id": "_K0KUmDVDQgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now select records based on our conditions. Records that don't fulfill the conditions will be discarded. This binary behavior (\"keep or not\") only gets us so far however. What if we want to transform the data in such a way that all records fulfilling a certain condition get mapped to 1 and all others to zero ? "
      ],
      "metadata": {
        "id": "EjoYUUQeHLK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Conclusion\n",
        "\n",
        "You learned how to perform some basic transformations of the table, but maybe you also want to apply more complex functions to the dataframe's rows or columns such as summary statistics. In the next chapter we are going to take a look at advanced transformations."
      ],
      "metadata": {
        "id": "RcEXL7sKmO2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Functions and  Advanced Transformations\n",
        "Advanced transformations are where PySpark really shines enabling us to execute very complex queries using simple syntax to extract valuable insights from our data. In this chapter we will see the power of methods such as `.groupBy()`, `.join()` especially in combination with more complex functions that are provided by the `functions` module. "
      ],
      "metadata": {
        "id": "xOWsFQy4cDM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Intro to Spark functions ?\n",
        "In general it is possible to use functions from other libraries such as `numpy` on Spark `DataFrame` objects, however this defeats the purpose of Spark which is its ability to optimize the performance of transformation pipelines due to lazy execution. \n",
        "\n",
        "This is why the `functions` exists which provides use with a copious amount of functions for all kinds of purposes.\n",
        "\n",
        "Suppose we want to map all record in the iris dataset whose petal width is above let's say 0.5 to one and all others to zero. Although you wouldn't think of such an if-else statement as a function at first in Python in PySpark it is implemented in such a way through the function `.when()`.\n"
      ],
      "metadata": {
        "id": "Ef25TfNJkhfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "w = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "\n",
        "(df1.select(\"species\",\n",
        "            F.when(df1[\"petalLength\"] > 0.5 ,1)\n",
        "             .otherwise(0).alias(\"pL above 0.5\"))\n",
        "    .show(5))"
      ],
      "metadata": {
        "id": "gxUxGZYJbnFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now 0.5 is a pretty arbitrary value. Something like the mean or median would likely be more interesting. You might be tempted to write something like `df1[\"petalLength\"] > F.mean(\"petalLength\")` as the first argument of `F.when()` in line `7` above. This however will get you into \"*Teufels Küche*\" i.e one hell of a mess.\n",
        "\n",
        "The problem is the following functions such as `F.mean()` aggregate the values of a column into one value, but in our case we want to compare each value of the \"petalLength\" column with the mean of the column.\n",
        "\n",
        "In order to deal with such dimensional mismatches we can use window functions, the way they are combined with `F.mean()` might look a bit unintuitive but what they do is basically return the mean, or the result of any other aggregation function, in form of a column containing as many rows as desired. In our case we want to have as many rows as there are in the `df1.petalWidth`. Maybe a look at the graphic before the code will help you to get an even better understanding of what's going on."
      ],
      "metadata": {
        "id": "fa-pSqhY2p9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "w = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "\n",
        "(df1.select(\"species\",\n",
        "            F.when(df1[\"petalLength\"] > F.mean(\"petalLength\").over(w) ,1)\n",
        "             .otherwise(0).alias(\"pL above mean\"))\n",
        "            .sample(fraction=0.1).show(5))"
      ],
      "metadata": {
        "id": "gKXcTbrc7klC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now what happens if the function we want to apply simply doesn't exist? After all the `functions` module contains only a finite amount of functions and chaining and combining them will only get us so far. This is where *User Defined Functions (UDFs)* come into play. Buckle up!"
      ],
      "metadata": {
        "id": "gt2KgBAFarNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. User Defined Functions\n",
        "\n",
        "By allowing us to define their own functions Spark gives us a lot of flexibility. After defining a Python function we can register it in our Spark session via `spark.udf.register()`. It can then be used in all Spark pipelines just as functions from the `functions` module would be used. \n",
        "\n"
      ],
      "metadata": {
        "id": "bmSUzPKzhZbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def firstUpper(s: str) -> str:\n",
        "    s  = s[0].upper() + s[1:]\n",
        "    return s\n",
        "\n",
        "firstUpper_UDF = F.udf(firstUpper, \"STRING\")\n",
        "# Apply function to our DataFrame contraining the Iris data.\n",
        "df1.select(firstUpper_UDF(\"species\").alias(\"Species\")).show(2)"
      ],
      "metadata": {
        "id": "sZ3Kyft-iQ7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `firstUpper` we just wrote is quite trivial. From a code style perspective it would be appropriate to substitute it via an anonymus function. Annonymus functions in Python can be created via the `lambda` keyword. "
      ],
      "metadata": {
        "id": "t2c56EaZ0lV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "firstUpper_UDF = F.udf(lambda s: s[0].upper() + s[1:], \"STRING\")\n",
        "# Apply function to our DataFrame contraining the Iris data.\n",
        "df1.select(firstUpper_UDF(\"species\").alias(\"Species\")).show(2)"
      ],
      "metadata": {
        "id": "O9KauDto0jH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you know, there is no free lunch. The flexibility that comes with UDF's come with a performance trade-off, since UDF's are a black-box to PySpark it can't use the optimization engines used for optimizing the exectution of functions from the `functions` module. For this reason you should use the functions from the `functions` module whenever possible.\n",
        "\n",
        "Sometimes we don't want to apply functions to all of the entries in a column but rather to a subset. An important way to generate such subsets is the `groupBy()` transformation which we will see in action up next."
      ],
      "metadata": {
        "id": "EYFQUfi817ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Grouping Values by Attribute.\n",
        "What if we want to know how many plants belong to each species in our data set ? `.describe()` from section *1.1* didn't give us that information. In section *3.1* we've seen how to get the mean of the column of a data set specifically the mean petalWidth of the iris flower. What if we want to get the specific mean of a certain species of iris flower ?\n",
        "\n",
        "This is where the `.groupBy()` transformation comes into play. It assigns entries of a data set to groups based on the values of those entries in a specific column. Afterwards so-called *aggregation* functions can be applied to each group which map the group to a single object, usually a numerical value such as the mean.\n",
        "\n",
        "Figuring out how many plants belong to each species is an example of aggregation based on the same column by which the values are grouped.\n",
        "\n"
      ],
      "metadata": {
        "id": "KWGmm_0f8zVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"species\")    # Consider the column \"species\" [string].\n",
        "    .groupBy(\"species\")   # Identical entries are assigned to the same group.\n",
        "    .count()              # For each group: count the number of entries belonging to it.\n",
        "    .orderBy(\"count\")     \n",
        "    .show(n=3))"
      ],
      "metadata": {
        "id": "HkeYICpksKbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the mean petal width for each species could be done  in a similar fashion to the code above. In the case of the mean however we might also want to round our results and give them a concise name. Maybe we are also interested in knowing the maximum petal width or other summary statistics of each species petal width.\n",
        "\n",
        "In general the `.agg()` transformation takes a list or dictionary of aggregation functions as input and applies them to the groups created by `.groupBy()`. A typical workflow could look like this:\n",
        "\n"
      ],
      "metadata": {
        "id": "sDZA70U33JUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df1.select(\"petalWidth\", \"species\")            # Consider the two columns \"petalWidth\" [double] and \"species\" [string].\n",
        "    .where(F.col(\"petalWidth\").isNotNull())     # Only consider entries where the petalWidth attribute is not Null.\n",
        "    .groupBy(\"species\")                         # Group the entries in the petalWidth column by their species.\n",
        "    .agg(F.round(F.mean(\"petalWidth\"),2)        # Calculate the mean petalWidth of each species rounded to two digits.\n",
        "          .alias(\"mean_pW\"),\n",
        "         F.max(\"petalWidth\"))                   # Name resulting column containing the means \"mean_pW\".\n",
        "    .orderBy(\"mean_pW\", ascending=False)        \n",
        "    .show(n=3))"
      ],
      "metadata": {
        "id": "S3MfZN08_i9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've been introduced to `.count()` and `.groupBy()` we can even specify our analysis of the petal Length from *3.1*. We can now find out the number of plants from each species whose petal length is above the mean petal length for the respective species. This section of code is probably the most complicated one in the tutorial but pondering over it will give your understanding of the material a huge boost so take a good look!"
      ],
      "metadata": {
        "id": "bsdjNnSE9B3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = Window.partitionBy(\"species\")                                               # Apply window species-wise.\n",
        "\n",
        "(df1.select(\"species\",                                                         \n",
        "            F.when(df1.petalLength > F.mean(\"petalLength\").over(w), 1)          \n",
        "             .otherwise(0).alias(\"pL above species mean\"))                      \n",
        "    .groupBy(\"species\")\n",
        "    .agg((F.sum(\"pL above species mean\") / F.count(\"species\"))                       \n",
        "          .alias(\"pL larger than species mean (%)\"))\n",
        "    .show())"
      ],
      "metadata": {
        "id": "Vrv9GbaX-SdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that was a bit tougher than the concepts in the second  chapter, after all we are dealing with *advanced* transformations. The next and final  concept we will learn about is the `.join()` that some of you might know from working with SQL."
      ],
      "metadata": {
        "id": "frsUYiambPoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Joining DataFrames.\n",
        "Suppose we have a set of *entities* (flowers in our concrete example) whoose *attributes* are spread over multiple datasets. This means that one dataset might contain the petal width and petal length and another the type of species. A logical thing to do would be to combine *join* the datasets (which are our  DataFrames) into one  big dataset containing all attributes associated with an entity.\n",
        "\n",
        "Before joining two DataFrames in PySpark we have to decide on two things: Based on which attributes to join the two DataFrames and how to join them. In simple cases  both DataFrames contain an ID column which can easily be used to join  records from  both DataFrames.\n",
        "\n",
        "The question \"how to join\" arises when the DataFrames don't contain the same records. We could decide to only consider records corresponding to IDs that are contained in both columns. This would lead to loss of information however, since we would have to discard records that contain non-Null values. Alternatively we could fill the columns from the DataFrame in which the ID isn't contained with Null-values. \n",
        "\n",
        "The approaches we have described here can be selected in `.join()` by setting `how=\"inner\"` or `how=\"full_outer\"` respectively. There are other types of joins such as *left joins* and *right joins* as well as others which make the `.join()` transformation a potent tool for data combination.\n",
        "\n",
        "The idea behind joins can be neatly visualized via Venn-diagrams. Take a look at the graphic before running the code cell below.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TAZPXRE6csuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/\n",
        "\n",
        "# Since the iris dataset isn't very useful for illustrating joins\n",
        "# you can take the first four sections of this tutorial as a \n",
        "# foundation for a code cell applying .join to the Jeremy Clarkson data."
      ],
      "metadata": {
        "id": "Ulo2LE4GlheF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Conclusion.\n",
        "\n",
        "Nice! You went through some pretty serious stuff! All in all you learned how to extract very specific information from data via functions, windows and `groupBy()` transformations as well as flexibly merging DataFrames via `.join()`. In the final chapter you will see how to perform the final step of an ETL workflow and that is loading the extracted and transformed data from the staging are into a data warehouse. "
      ],
      "metadata": {
        "id": "KsyEfzy2xnNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Save results to database.\n",
        "\n",
        "The easiest way to save the data is to convert the dataframe to a pandas dataframe and have pandas generate all SQL statements by itself."
      ],
      "metadata": {
        "id": "f19m3l4TyN5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rg8LlpPuhYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "connection = sqlite3.connect('my-database.sqlite')\n",
        "\n",
        "# Please note that tuples with a single value always end with a comma.\n",
        "# e.g. ('Max', ). If that comma does not exist, the value will be seen as a string\n",
        "# instead of a tuple.\n",
        "frame = spark.createDataFrame(data=[('Max', ), ('Erika', )], schema=['name'])\n",
        "frame.toPandas().to_sql('table_name', connection, if_exists='replace', index=True)\n",
        "\n",
        "print(connection.execute('SELECT * FROM table_name;').fetchall())\n",
        "\n",
        "connection.close()"
      ],
      "metadata": {
        "id": "eVjVbGdTzdUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JE9O9ScfxnaO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}